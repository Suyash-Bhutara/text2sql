from typing import List, Dict, Any, TypedDict, Annotated
from datetime import datetime, timezone

from langchain_core.messages import BaseMessage
from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool

from .llm import llm_instance
from .database import db_instance
from .prompts import (
    query_generation_prompt,
    answer_generation_prompt,
    format_chat_history_for_prompt,
)

from .config import DEFAULT_TOP_K_RESULTS


# --- State Definition ---
class State(TypedDict):
    question: str
    chat_history: List[BaseMessage]  # Stores Langchain message objects
    query: str  # SQL query generated by LLM
    result: str  # Result from SQL query execution
    answer: str  # Final natural language answer
    # Optional fields for handling clarification
    clarification_needed: bool 
    clarification_question: str

# --- TypedDict for query output structure ---
class QueryOutput(TypedDict):
    """Generated SQL query."""
    query: Annotated[str, ..., "Syntactically valid SQL query."]


def write_query(state: State) -> Dict[str, Any]:
    """
    Generates an SQL query based on the user's question and chat history.
    """
    print("--- Node: write_query ---")
    question = state["question"]
    chat_history = state.get("chat_history", [])

    current_date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    prompt_input = {
        "dialect": db_instance.dialect,
        "top_k": DEFAULT_TOP_K_RESULTS,
        "table_info": db_instance.get_table_info(),  # Get fresh table info
        "input": question,
        "chat_history": chat_history,
        "current_date": current_date_str,
    }

    # Create the actual prompt messages
    formatted_prompt_messages = query_generation_prompt.invoke(prompt_input)

    # Use with_structured_output to ensure the LLM returns a dict with a "query" key
    structured_llm_query_gen = llm_instance.with_structured_output(QueryOutput)


    try:
        ai_response_obj = structured_llm_query_gen.invoke(formatted_prompt_messages)
        generated_query = ai_response_obj["query"]

        # Check for clarification needed signal from LLM
        if "CLARIFICATION_NEEDED:" in generated_query:
            clarification_question = generated_query.replace(
                "CLARIFICATION_NEEDED:", ""
            ).strip()
            print(f"Clarification needed: {clarification_question}")
            return {
                "query": generated_query,
                "clarification_needed": True,
                "clarification_question": clarification_question,
            }
        return {"query": generated_query, "clarification_needed": False}
    except Exception as e:
        print(f"Error in write_query: {e}")
        # Return an error state or a query that indicates an error
        return {"query": f"ERROR_GENERATING_QUERY: {e}", "clarification_needed": False}


def execute_query(state: State) -> Dict[str, str]:
    """
    Executes the generated SQL query against the database.
    """
    print("--- Node: execute_query ---")
    query = state["query"]

    if state.get("clarification_needed") or "ERROR_GENERATING_QUERY:" in query:
        print(
            "Skipping query execution due to clarification needed or query generation error."
        )
        return {
            "result": (
                "Query execution skipped."
                if not "ERROR_GENERATING_QUERY:" in query
                else query
            )
        }

    execute_query_tool = QuerySQLDatabaseTool(db=db_instance)

    try:
        query_result = execute_query_tool.invoke(query)
        print(f"SQL Query Result: \n{query_result}")
        return {"result": str(query_result)}
    except Exception as e:
        print(f"Error executing SQL query: {e}")
        return {"result": f"ERROR_EXECUTING_QUERY: {e}"}


def generate_answer(state: State) -> Dict[str, str]:
    """
    Generates a natural language answer based on the question, query, and result.
    """
    print("--- Node: generate_answer ---")
    question = state["question"]
    query = state["query"]
    # This will contain data or an error message from execute_query
    result = state["result"]
    chat_history = state.get("chat_history", [])

    # If clarification was needed and execution was skipped, the answer should reflect that.
    if state.get("clarification_needed"):
        clarification_question = state.get(
            "clarification_question", "I need more details to proceed."
        )
        print(f"Returning clarification request: {clarification_question}")
        return {"answer": clarification_question}

    if "ERROR_GENERATING_QUERY:" in query:
        error_message = query.replace("ERROR_GENERATING_QUERY:", "").strip()
        answer = f"I encountered an issue trying to understand your request: {error_message}. Could you please rephrase?"
        print(f"Answer due to query generation error: {answer}")
        return {"answer": answer}

    # Format chat history for inclusion in the prompt
    chat_history_str = format_chat_history_for_prompt(chat_history)

    # Prepare the input for the answer generation prompt
    prompt_input = {
        "chat_history_str": chat_history_str,
        "question": question,
        "query": query,
        "result": result,
    }

    formatted_prompt_messages = answer_generation_prompt.invoke(prompt_input)

    try:
        # Invoke the LLM for answer generation
        ai_response_obj = llm_instance.invoke(formatted_prompt_messages)
        natural_language_answer = ai_response_obj.content
        print(f"Generated Natural Language Answer: \n{natural_language_answer}")
        return {"answer": natural_language_answer}
    except Exception as e:
        print(f"Error in generate_answer: {e}")
        return {
            "answer": f"Sorry, I encountered an issue while formulating the final answer: {e}"
        }
